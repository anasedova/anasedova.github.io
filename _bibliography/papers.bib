---
---

@inproceedings{sedova-etal-2024-know,
    title={To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity},
    author={Sedova, Anastasiia and Litschko, Robert  and Frassinelli, Diego  and Roth, Benjamin  and Plank, Barbara},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
    month={nov},
    year={2024},
    address={Miami, Florida, USA},
    publisher={Association for Computational Linguistics},
    url={https://aclanthology.org/2024.findings-emnlp.1003/},
    doi={10.18653/v1/2024.findings-emnlp.1003},
    pages={17203--17217},
    abstract={One of the major aspects contributing to the striking performance of large language models (LLMs) is the vast amount of factual knowledge accumulated during pre-training. Yet, many LLMs suffer from self-inconsistency, which raises doubts about their trustworthiness and reliability. This paper focuses on entity type ambiguity, analyzing the proficiency and consistency of state-of-the-art LLMs in applying factual knowledge when prompted with ambiguous entities. To do so, we propose an evaluation protocol that disentangles knowing from applying knowledge, and test state-of-the-art LLMs on 49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing the correct entity reading, achieving an average accuracy of only 85{\%}, and as low as 75{\%} with underspecified prompts. The results also reveal systematic discrepancies in LLM behavior, showing that while the models may possess knowledge, they struggle to apply it consistently, exhibit biases toward preferred readings, and display self-inconsistencies. This highlights the need to address entity ambiguity in the future for more trustworthy LLMs.},
    pdf={https://aclanthology.org/2024.findings-emnlp.1003.pdf},
    preview={to-know-or-not-to-know.png},
    slides={Know_not_know_slides.pdf},
    code={https://github.com/anasedova/ToKnow_or_NotToKnow},
    poster={emnlp23_know_not_know_poster.pdf},
    selected={true},
    bibtex_show = {true},
}

@inproceedings{Sedova_2023,
   title={Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal},
   author={Sedova, Anastasiia and Zellinger, Lena and Roth, Benjamin},
   booktitle={Machine Learning and Knowledge Discovery in Databases: Research Track},
   year={2023},
   ISBN={9783031434129},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-031-43412-9_14},
   DOI={10.1007/978-3-031-43412-9_14},
   publisher={Springer Nature Switzerland},
   pages={237â€“253},
   abstract={An accurate and substantial dataset is essential for training a reliable and well-performing model. However, even manually annotated datasets contain label errors, not to mention automatically labeled ones. Previous methods for label denoising have primarily focused on detecting outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for learning with noisy labels by using Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is dynamically adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates AGRA's effectiveness, while a comprehensive results analysis supports our initial hypothesis: permanent hard outlier removal is not always what model benefits the most from.},
   pdf={https://arxiv.org/abs/2306.04502},
   preview={agra_preview.png},
   slides={AGRA_pres.pdf},
   poster={AGRA_poster.pdf},
   code={https://github.com/anasedova/AGRA},
   selected={true},
}

@inproceedings{sedova-roth-2023-actc,
    title = "{ACTC}: Active Threshold Calibration for Cold-Start Knowledge Graph Completion",
    author = "Sedova, Anastasiia  and
      Roth, Benjamin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.158/",
    doi = "10.18653/v1/2023.acl-short.158",
    pages = "1853--1863",
    abstract = "Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotator show an improvement of 7{\%} points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4{\%} points over different budgets.",
    pdf={https://aclanthology.org/2023.acl-short.158.pdf},
    preview={actc_preview.png},
    slides={ACTC_pres.pdf},
    poster={actc_poster.pdf},
    code={https://github.com/anasedova/ACTC},
    selected={true}

}

@inproceedings{sedova-etal-2021-knodle,
    title="Knodle: Modular Weakly Supervised Learning with {P}y{T}orch",
    author="Sedova, Anastasiia  and
      Stephan, Andreas  and
      Speranskaya, Marina  and
      Roth, Benjamin",
    editor="Rogers, Anna  and
      Calixto, Iacer  and
      Vuli{\'c}, Ivan  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Camburu, Oana-Maria  and
      Bansal, Trapit  and
      Shwartz, Vered",
    booktitle="Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month=aug,
    year="2021",
    address="Online",
    publisher="Association for Computational Linguistics",
    url="https://aclanthology.org/2021.repl4nlp-1.12/",
    doi="10.18653/v1/2021.repl4nlp-1.12",
    pages="100--111",
    abstract="Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.",
    pdf={https://aclanthology.org/2021.repl4nlp-1.12.pdf},
    preview={knodle.png},
    code={https://github.com/knodle/knodle},
    selected={true}
}


@inproceedings{kougia-etal-2024-analysing,
    title = "Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency",
    author = "Kougia, Vasiliki  and
      Sedova, Anastasiia  and
      Stephan, Andreas Joseph  and
      Zaporojets, Klim  and
      Roth, Benjamin",
    editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Miwa, Makoto  and
      Roberts, Kirk  and
      Tsujii, Junichi",
    booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.bionlp-1.6/",
    doi = "10.18653/v1/2024.bionlp-1.6",
    pages = "72--84",
    abstract = "This paper presents the first study for temporal relation extraction in a zero-shot setting focusing on biomedical text. We employ two types of prompts and five Large Language Models (LLMs; GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) to obtain responses about the temporal relations between two events. Our experiments demonstrate that LLMs struggle in the zero-shot setting, performing worse than fine-tuned specialized models in terms of F1 score. This highlights the challenging nature of this task and underscores the need for further research to enhance the performance of LLMs in this context. We further contribute a novel comprehensive temporal analysis by calculating consistency scores for each LLM. Our findings reveal that LLMs face challenges in providing responses consistent with the temporal properties of uniqueness and transitivity. Moreover, we study the relation between the temporal consistency of an LLM and its accuracy, and whether the latter can be improved by solving temporal inconsistencies. Our analysis shows that even when temporal consistency is achieved, the predictions can remain inaccurate.",
    pdf = {https://aclanthology.org/2024.bionlp-1.6.pdf},
    preview={temp_re_preview.png},
    code={https://github.com/vasilikikou/consistent_bioTempRE}
}

@misc{xia2024exploringpromptselicitmemorization,
      title={Exploring prompts to elicit memorization in masked language model-based named entity recognition},
      author={Yuxi Xia and Anastasiia Sedova and Pedro Henrique Luz de Araujo and Vasiliki Kougia and Lisa NuÃŸbaumer and Benjamin Roth},
      year={2024},
      eprint={2405.03004},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.03004},
      pdf = {https://arxiv.org/pdf/2405.03004},
    preview={memorization_preview.png},
    code = {https://github.com/Yuuxii/NER-memorization-detection},
}



@inproceedings{10.1145/3143699.3143734,
    author = {Mitrofanova, Olga and Sedova, Anastasiia},
    title = {Topic Modelling in Parallel and Comparable Fiction Texts (the case study of English and Russian prose)},
    year = {2017},
    isbn = {9781450354370},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3143699.3143734},
    doi = {10.1145/3143699.3143734},
    abstract = {The paper is devoted to processing parallel and comparable corpora by means of topic modelling. We focus our attention on Russian and English parallel and comparable texts. We use Latent Dirichlet Allocation (LDA) algorithm for building topic models of fiction texts, evaluation of compatibility for the original text and its translation(s), selection of possible translation equivalents.},
    booktitle = {Proceedings of the International Conference IMS-2017},
    pages = {175â€“180},
    numpages = {6},
    keywords = {English, Fiction, Parallel and Comparable Texts, Russian, Text Corpora, Topic Modelling},
    location = {Saint Petersburg, Russian Federation},
    series = {IMS2017}
}


