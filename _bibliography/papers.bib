---
---

@inproceedings{sedova-etal-2024-know,
    title={To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity},
    author={Sedova, Anastasiia and Litschko, Robert  and Frassinelli, Diego  and Roth, Benjamin  and Plank, Barbara},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
    month={nov},
    year={2024},
    address={Miami, Florida, USA},
    publisher={Association for Computational Linguistics},
    url={https://aclanthology.org/2024.findings-emnlp.1003/},
    doi={10.18653/v1/2024.findings-emnlp.1003},
    pages={17203--17217},
    abstract={One of the major aspects contributing to the striking performance of large language models (LLMs) is the vast amount of factual knowledge accumulated during pre-training. Yet, many LLMs suffer from self-inconsistency, which raises doubts about their trustworthiness and reliability. This paper focuses on entity type ambiguity, analyzing the proficiency and consistency of state-of-the-art LLMs in applying factual knowledge when prompted with ambiguous entities. To do so, we propose an evaluation protocol that disentangles knowing from applying knowledge, and test state-of-the-art LLMs on 49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing the correct entity reading, achieving an average accuracy of only 85{\%}, and as low as 75{\%} with underspecified prompts. The results also reveal systematic discrepancies in LLM behavior, showing that while the models may possess knowledge, they struggle to apply it consistently, exhibit biases toward preferred readings, and display self-inconsistencies. This highlights the need to address entity ambiguity in the future for more trustworthy LLMs.},
    pdf={https://aclanthology.org/2024.findings-emnlp.1003.pdf},
    preview={to-know-or-not-to-know.png},
    slides={Know_not_know_slides.pdf},
    code={https://github.com/anasedova/ToKnow_or_NotToKnow},
    poster={emnlp23_know_not_know_poster.pdf},
    selected={true}
}

@inbook{Sedova_2023,
   title={Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal},
   ISBN={9783031434129},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-031-43412-9_14},
   DOI={10.1007/978-3-031-43412-9_14},
   booktitle={Machine Learning and Knowledge Discovery in Databases: Research Track},
   publisher={Springer Nature Switzerland},
   author={Sedova, Anastasiia and Zellinger, Lena and Roth, Benjamin},
   year={2023},
   pages={237â€“253},
   selected={true},
   abstract={An accurate and substantial dataset is essential for training a reliable and well-performing model. However, even manually annotated datasets contain label errors, not to mention automatically labeled ones. Previous methods for label denoising have primarily focused on detecting outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for learning with noisy labels by using Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is dynamically adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates AGRA's effectiveness, while a comprehensive results analysis supports our initial hypothesis: permanent hard outlier removal is not always what model benefits the most from.},
   pdf={https://arxiv.org/abs/2306.04502},
   preview={agra_preview.png},
   slides={AGRA_pres.pdf},
   poster={AGRA_poster.pdf},
   code={https://github.com/anasedova/AGRA},
}

@inproceedings{sedova-roth-2023-actc,
    title = "{ACTC}: Active Threshold Calibration for Cold-Start Knowledge Graph Completion",
    author = "Sedova, Anastasiia  and
      Roth, Benjamin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.158/",
    doi = "10.18653/v1/2023.acl-short.158",
    pages = "1853--1863",
    abstract = "Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotator show an improvement of 7{\%} points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4{\%} points over different budgets.",
    pdf={https://aclanthology.org/2023.acl-short.158.pdf},
    preview={actc_preview.png},
    slides={ACTC_pres.pdf},
    poster={actc_poster.pdf},
    code={https://github.com/anasedova/ACTC}

}

@inproceedings{sedova-etal-2021-knodle,
    title="Knodle: Modular Weakly Supervised Learning with {P}y{T}orch",
    author="Sedova, Anastasiia  and
      Stephan, Andreas  and
      Speranskaya, Marina  and
      Roth, Benjamin",
    editor="Rogers, Anna  and
      Calixto, Iacer  and
      Vuli{\'c}, Ivan  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Camburu, Oana-Maria  and
      Bansal, Trapit  and
      Shwartz, Vered",
    booktitle="Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month=aug,
    year="2021",
    address="Online",
    publisher="Association for Computational Linguistics",
    url="https://aclanthology.org/2021.repl4nlp-1.12/",
    doi="10.18653/v1/2021.repl4nlp-1.12",
    pages="100--111",
    abstract="Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.",
    pdf={https://aclanthology.org/2021.repl4nlp-1.12.pdf},
    preview={knodle.png},
    code={https://github.com/knodle/knodle}

}


